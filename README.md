# Fine Tuning Repo

Welcome to the Fine Tuning Repo! This repository is dedicated to exploring various aspects of fine-tuning machine learning models. As I continue to learn and expand my understanding, this repo will evolve with more content and insights.

## Contents

### 1. PEFT (Performance Efficient Fine-Tuning)
- **Quantization**: Techniques to reduce the precision of the numbers used in model computations, making models faster and more efficient.
- **LoRA (Lower Order Rank Adaptation)**: A method to adapt pre-trained models to new tasks with minimal computational resources.

### 2. Understanding GPU / CPU / TPUs
- Detailed explanations of the differences, advantages, and use cases for GPUs, CPUs, and TPUs in machine learning.

### 3. Data Preparation (Instruction Set)
- Guidelines and best practices for preparing data for fine-tuning, including data cleaning, augmentation, and formatting.

### 4. Fine Tuning
- Step-by-step instructions and methodologies for fine-tuning pre-trained models on new datasets.

### 5. Evaluation
- Techniques and metrics for evaluating the performance of fine-tuned models.

## Note
This repository will evolve as I add more content and increase my understanding of each aspect of fine-tuning.

## History
- **April 2025**: Repurposed this repo for internal team training.

Feel free to contribute, raise issues, or suggest improvements. Happy fine-tuning!