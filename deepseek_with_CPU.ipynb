{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d675e49c",
   "metadata": {},
   "source": [
    "## Remember!!! Even this is big model for CPU based machines.\n",
    "### Install required modules\n",
    "Use existing package managers (Conda, UV, Pip) to install required modules.\n",
    "Ran this model on a CPU based Server, with 64 GB RAM and for inferencing CPU as 100% for more than 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd39a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,BitsAndBytesConfig\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a8c37",
   "metadata": {},
   "source": [
    "### Check version of Torch and is Torch enabled with GPU.\n",
    "CUDA libraries are developed by NVidia and Pytorch are python abstractions over NVidia CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1837d979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.6.0+cpu\n",
      "GPU enabled with Pytorch:  False\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "print(f\"GPU enabled with Pytorch:  {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8face93a",
   "metadata": {},
   "source": [
    "### Hugging Face API\n",
    "1. Create Hugging Face Account if not already exists.\n",
    "2. Create API Token\n",
    "3. Configure token in .env file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7142894",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51de48b",
   "metadata": {},
   "source": [
    "Function: Load Model\n",
    "1. Given a model name\n",
    "2. From HF model hub, loads the model in memory.\n",
    "\n",
    "Note: \n",
    "1. When model is loaded it uses GPU / CPU based on avilable compute resources.\n",
    "2. By default, pytorch uses datatype of weights as FP32.\n",
    "3. On GPUs, loading models may fail if they exceed GPU memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8bb3cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"):\n",
    "    model_name = model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, token=token)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab96646c",
   "metadata": {},
   "source": [
    "Load Model in Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51e40bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\"unsloth/DeepSeek-R1-Distill-Llama-8B\")\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce067f2",
   "metadata": {},
   "source": [
    "Lets undersand details of model.\n",
    "1. Number of parameters or weights\n",
    "2. Datatype of weights.\n",
    "3. CPU / GPU based compute\n",
    "4. Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03479fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 8030261248\n",
      "Approximate (Original) model size: 30 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of model parameters: {model.num_parameters()}\")\n",
    "print(f\"Approximate (Original) model size: {round(model.get_memory_footprint()/1024/1024/1024)} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe6e6a6",
   "metadata": {},
   "source": [
    "Important things to note here\n",
    "1. Layers (number of layers)\n",
    "2. Data Type (Float32)\n",
    "3. Device: CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc838698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Model Layers: 32\n",
      "Model Embedding Size: 4096\n",
      "Model Device: cpu\n",
      "model.embed_tokens.weight torch.float32 cpu\n",
      "model.layers.0.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.0.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.0.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.0.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.0.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.0.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.0.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.0.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.0.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.1.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.1.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.1.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.1.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.1.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.1.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.1.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.1.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.1.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.2.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.2.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.2.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.2.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.2.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.2.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.2.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.2.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.2.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.3.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.3.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.3.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.3.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.3.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.3.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.3.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.3.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.3.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.4.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.4.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.4.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.4.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.4.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.4.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.4.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.4.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.4.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.5.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.5.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.5.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.5.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.5.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.5.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.5.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.5.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.5.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.6.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.6.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.6.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.6.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.6.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.6.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.6.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.6.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.6.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.7.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.7.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.7.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.7.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.7.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.7.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.7.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.7.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.7.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.8.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.8.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.8.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.8.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.8.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.8.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.8.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.8.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.8.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.9.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.9.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.9.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.9.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.9.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.9.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.9.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.9.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.9.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.10.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.10.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.10.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.10.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.10.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.10.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.10.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.10.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.10.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.11.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.11.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.11.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.11.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.11.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.11.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.11.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.11.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.11.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.12.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.12.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.12.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.12.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.12.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.12.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.12.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.12.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.12.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.13.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.13.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.13.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.13.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.13.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.13.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.13.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.13.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.13.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.14.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.14.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.14.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.14.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.14.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.14.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.14.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.14.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.14.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.15.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.15.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.15.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.15.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.15.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.15.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.15.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.15.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.15.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.16.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.16.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.16.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.16.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.16.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.16.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.16.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.16.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.16.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.17.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.17.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.17.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.17.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.17.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.17.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.17.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.17.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.17.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.18.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.18.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.18.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.18.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.18.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.18.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.18.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.18.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.18.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.19.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.19.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.19.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.19.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.19.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.19.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.19.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.19.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.19.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.20.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.20.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.20.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.20.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.20.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.20.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.20.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.20.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.20.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.21.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.21.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.21.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.21.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.21.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.21.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.21.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.21.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.21.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.22.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.22.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.22.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.22.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.22.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.22.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.22.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.22.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.22.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.23.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.23.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.23.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.23.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.23.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.23.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.23.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.23.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.23.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.24.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.24.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.24.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.24.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.24.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.24.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.24.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.24.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.24.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.25.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.25.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.25.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.25.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.25.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.25.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.25.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.25.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.25.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.26.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.26.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.26.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.26.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.26.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.26.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.26.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.26.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.26.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.27.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.27.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.27.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.27.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.27.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.27.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.27.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.27.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.27.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.28.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.28.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.28.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.28.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.28.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.28.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.28.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.28.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.28.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.29.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.29.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.29.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.29.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.29.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.29.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.29.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.29.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.29.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.30.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.30.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.30.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.30.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.30.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.30.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.30.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.30.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.30.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.31.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.31.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.31.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.31.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.31.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.31.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.31.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.31.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.31.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.norm.weight torch.float32 cpu\n",
      "lm_head.weight torch.float32 cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Model Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Model Embedding Size: {model.config.hidden_size}\")\n",
    "print(f\"Model Device: {model.device}\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype, param.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167b767f",
   "metadata": {},
   "source": [
    "\"generate_model_response\" generates responses from model. This function is used multiple times in the experiement.\n",
    "It takes model and tokenizer as parameters along with ineference parameters like Prompt(context), temperature, number outputs (k).\n",
    "1. Prompts are converted into Tokens with token_id.\n",
    "2. And measuring inference time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d735a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response(\n",
    "        prompt:str,\n",
    "        tokenizer:AutoTokenizer,\n",
    "        model:AutoModelForCausalLM,\n",
    "        max_length:int=3500,\n",
    "        temperature:float=0.1,\n",
    "        top_k:int=50)->str:\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\",padding=True)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    inputs = {k: v.to(device) for k, v in input_ids.items()}\n",
    "    attention_mask = input_ids[\"attention_mask\"]\n",
    "    input_ids = input_ids[\"input_ids\"]\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        output = model.generate(\n",
    "                                    **inputs, \n",
    "                                    max_length=max_length, \n",
    "                                    do_sample=True,\n",
    "                                    temperature=temperature, \n",
    "                                    top_k=top_k,\n",
    "                                    # attention_mask=attention_mask,\n",
    "                                    pad_token_id=pad_token_id,\n",
    "                                    eos_token_id=eos_token_id\n",
    "                                    )\n",
    "        final_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        print(final_output)\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken: {end_time-start_time}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade74816",
   "metadata": {},
   "source": [
    "Here the mode is down casted from Float32 to Brain (Google Brain) Float16. Refer below links for better understanding: \n",
    "1. https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus\n",
    "2. https://huggingface.co/docs/transformers/main/en/model_memory_anatomy#anatomy-of-models-memory\n",
    "3. https://huggingface.co/docs/transformers/main/en/quantization/overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a11be2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in BFloat16\n"
     ]
    }
   ],
   "source": [
    "model_bfloat16 = model.to(dtype=torch.bfloat16)\n",
    "# tokenizer_fp16 = tokenizer\n",
    "print(\"Model loaded in BFloat16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f71c292",
   "metadata": {},
   "source": [
    "Now the model size is approx 15 GB compared previous 30 GB but the number of parameters are identical. This effectively reduces the memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7cf963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Model Layers: 32\n",
      "Model Embedding Size: 4096\n",
      "Model Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Model Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Model Embedding Size: {model.config.hidden_size}\")\n",
    "print(f\"Model Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d9f47",
   "metadata": {},
   "source": [
    "Inference from in memory loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d75e312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,   3923,    374,    279,   7438,    315,   2324,     30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "What is the meaning of life? This is a question that has been pondered by countless individuals throughout history. Different cultures and philosophies have offered various answers, but none have been universally accepted. So, perhaps the meaning of life is something that each person has to discover for themselves. Let me explore some of the common perspectives on this age-old question.\n",
      "\n",
      "First, let's consider the philosophical perspective. In philosophy, thinkers like Socrates, Aristotle, and existentialists such as Sartre have all had their own takes on the meaning of life. Socrates believed that the unexamined life is not worth living, suggesting that self-examination and reflection are essential. Aristotle, on the other hand, proposed that the meaning of life is found in the pursuit of knowledge and virtue, aiming for a life of happiness and fulfillment through reason and moral excellence.\n",
      "\n",
      "Existentialists, such as Jean-Paul Sartre, argue that life has no inherent meaning, and it's up to each individual to create their own meaning through their choices and actions. This perspective emphasizes freedom and responsibility, suggesting that we are free to choose how we live, but also that we are fully responsible for the consequences of those choices.\n",
      "\n",
      "Religious perspectives often provide a more defined answer. Many religions posit that the meaning of life is to know, love, and serve a higher power or divine being. For example, in Christianity, the meaning of life is often expressed as a relationship with God, with the purpose of glorifying Him and living in accordance with His will. Similarly, in Buddhism, the purpose of life is seen as achieving enlightenment and escaping the cycle of suffering (samsara) through compassion and wisdom.\n",
      "\n",
      "In contrast, secular perspectives might look for meaning in life without invoking a divine being. This could involve finding purpose in relationships, personal growth, contributing to society, or pursuing happiness and fulfillment through personal achievements or helping others. Some might find meaning in creativity, art, or science, while others might focus on ethical living or fostering connections with others.\n",
      "\n",
      "Another angle is the psychological perspective, which examines how individuals perceive and construct their own meaning of life. Psychologists like Carl Jung have explored the concept of the \"self,\" suggesting that the meaning of life is about self-actualization and realizing one's potential. Viktor Frankl, a Holocaust survivor and psychiatrist, developed the concept of logotherapy, which posits that the primary drive in life is the search for meaning, and finding meaning can be therapeutic and life-affirming.\n",
      "\n",
      "It's also worth considering the perspective of science and evolution. From a scientific viewpoint, life is often seen as a product of natural processes, and the meaning of life might be understood in terms of survival and reproduction. However, this perspective doesn't necessarily provide a meaning, but rather explains how life functions.\n",
      "\n",
      "Moreover, some people might find meaning in the process of living itself—appreciating the beauty of the world, the joy of experiences, or the simple act of existence. This could be tied to a philosophy of stoicism, where one finds contentment in the present moment, regardless of external circumstances.\n",
      "\n",
      "It's important to note that different people might have different answers, and that's okay. There isn't a one-size-fits-all meaning of life. Some might find their purpose in helping others, while others might find it in personal achievements or creative expression. The key might be to explore what brings you joy, fulfillment, and a sense of purpose, and to align your life with that.\n",
      "\n",
      "In summary, the meaning of life is a deeply personal and subjective question. It can be approached through various lenses—philosophical, religious, psychological, and personal. Each individual has to reflect on their own beliefs, experiences, and values to discover what the meaning of life is for them.\n",
      "</think>\n",
      "\n",
      "The meaning of life is a profoundly personal and subjective question, varying across cultures, philosophies, and individuals. Here is a structured summary of the exploration:\n",
      "\n",
      "1. **Philosophical Perspectives**:\n",
      "   - **Socrates**: Emphasized self-examination and reflection as essential for a meaningful life.\n",
      "   - **Aristotle**: Suggested the pursuit of knowledge and virtue for a life of happiness and fulfillment.\n",
      "   - **Existentialism**: Argued that life lacks inherent meaning, urging individuals to create their own through choices and actions.\n",
      "\n",
      "2. **Religious Perspectives**:\n",
      "   - **Abrahamic Religions**: Often propose a relationship with a divine being as the central purpose, involving glorification and living according to divine will.\n",
      "   - **Buddhism**: Focuses on achieving enlightenment and escaping suffering through compassion and wisdom.\n",
      "\n",
      "3. **Secular Perspectives**:\n",
      "   - **Personal Growth**: Finding meaning in relationships, achievements, or helping others.\n",
      "   - **Creativity and Art**: Meaning can be derived from creativity, science, or ethical living.\n",
      "\n",
      "4. **Psychological Perspectives**:\n",
      "   - **Self-Actualization**: Carl Jung suggested realizing one's potential as central to life's meaning.\n",
      "   - **Logotherapy**: Viktor Frankl emphasized the search for meaning as therapeutic and life-affirming.\n",
      "\n",
      "5. **Scientific and Evolutionary Perspectives**:\n",
      "   - Often viewed through natural processes, focusing on survival and reproduction, though not necessarily providing a meaning.\n",
      "\n",
      "6. **Stoic Philosophy**:\n",
      "   - Finds contentment in the present moment, appreciating the beauty and joy of existence.\n",
      "\n",
      "**Conclusion**: The meaning of life is deeply personal, varying based on individual beliefs, experiences, and values. Each person must reflect on what brings them joy, fulfillment, and purpose, aligning their life accordingly. There is no universal answer, and exploring what resonates personally is key.\n",
      "Time taken: 448.76119589805603\n"
     ]
    }
   ],
   "source": [
    "generate_model_response(\"What is the meaning of life?\", tokenizer=tokenizer, model=model_bfloat16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
