{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d675e49c",
   "metadata": {},
   "source": [
    "## Remember!!! Even this is big model for CPU based machines.\n",
    "### Install required modules\n",
    "Use existing package managers (Conda, UV, Pip) to install required modules.\n",
    "Ran this model on a CPU based Server, with 64 GB RAM and for inferencing CPU as 100% for more than 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd39a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,BitsAndBytesConfig\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a8c37",
   "metadata": {},
   "source": [
    "### Check version of Torch and is Torch enabled with GPU.\n",
    "CUDA libraries are developed by NVidia and Pytorch are python abstractions over NVidia CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1837d979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 2.6.0+cpu\n",
      "GPU enabled with Pytorch:  False\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "print(f\"GPU enabled with Pytorch:  {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8face93a",
   "metadata": {},
   "source": [
    "### Hugging Face API\n",
    "1. Create Hugging Face Account if not already exists.\n",
    "2. Create API Token\n",
    "3. Configure token in .env file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7142894",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51de48b",
   "metadata": {},
   "source": [
    "Function: Load Model\n",
    "1. Given a model name\n",
    "2. From HF model hub, loads the model in memory.\n",
    "\n",
    "Note: \n",
    "1. When model is loaded it uses GPU / CPU based on avilable compute resources.\n",
    "2. By default, pytorch uses datatype of weights as FP32.\n",
    "3. On GPUs, loading models may fail if they exceed GPU memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8bb3cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"):\n",
    "    model_name = model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, token=token)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab96646c",
   "metadata": {},
   "source": [
    "Load Model in Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51e40bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [03:03<00:00, 45.81s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\"unsloth/DeepSeek-R1-Distill-Llama-8B\")\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce067f2",
   "metadata": {},
   "source": [
    "Lets undersand details of model.\n",
    "1. Number of parameters or weights\n",
    "2. Datatype of weights.\n",
    "3. CPU / GPU based compute\n",
    "4. Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03479fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 8030261248\n",
      "Approximate model size: 29.915054321289062 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of model parameters: {model.num_parameters()}\")\n",
    "print(f\"Approximate model size: {model.num_parameters()*4/1024/1024/1024} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc838698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.float32 cpu\n",
      "model.layers.0.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.0.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.0.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.0.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.0.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.0.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.0.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.0.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.0.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.1.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.1.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.1.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.1.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.1.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.1.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.1.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.1.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.1.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.2.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.2.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.2.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.2.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.2.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.2.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.2.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.2.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.2.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.3.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.3.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.3.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.3.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.3.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.3.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.3.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.3.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.3.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.4.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.4.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.4.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.4.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.4.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.4.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.4.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.4.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.4.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.5.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.5.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.5.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.5.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.5.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.5.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.5.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.5.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.5.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.6.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.6.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.6.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.6.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.6.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.6.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.6.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.6.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.6.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.7.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.7.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.7.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.7.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.7.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.7.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.7.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.7.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.7.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.8.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.8.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.8.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.8.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.8.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.8.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.8.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.8.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.8.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.9.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.9.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.9.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.9.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.9.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.9.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.9.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.9.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.9.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.10.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.10.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.10.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.10.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.10.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.10.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.10.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.10.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.10.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.11.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.11.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.11.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.11.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.11.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.11.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.11.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.11.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.11.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.12.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.12.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.12.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.12.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.12.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.12.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.12.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.12.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.12.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.13.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.13.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.13.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.13.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.13.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.13.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.13.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.13.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.13.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.14.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.14.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.14.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.14.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.14.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.14.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.14.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.14.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.14.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.15.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.15.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.15.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.15.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.15.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.15.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.15.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.15.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.15.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.16.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.16.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.16.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.16.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.16.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.16.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.16.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.16.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.16.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.17.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.17.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.17.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.17.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.17.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.17.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.17.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.17.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.17.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.18.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.18.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.18.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.18.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.18.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.18.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.18.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.18.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.18.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.19.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.19.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.19.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.19.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.19.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.19.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.19.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.19.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.19.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.20.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.20.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.20.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.20.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.20.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.20.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.20.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.20.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.20.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.21.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.21.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.21.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.21.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.21.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.21.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.21.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.21.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.21.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.22.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.22.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.22.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.22.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.22.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.22.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.22.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.22.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.22.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.23.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.23.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.23.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.23.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.23.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.23.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.23.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.23.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.23.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.24.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.24.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.24.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.24.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.24.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.24.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.24.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.24.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.24.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.25.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.25.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.25.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.25.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.25.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.25.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.25.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.25.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.25.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.26.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.26.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.26.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.26.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.26.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.26.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.26.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.26.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.26.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.27.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.27.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.27.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.27.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.27.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.27.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.27.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.27.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.27.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.28.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.28.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.28.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.28.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.28.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.28.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.28.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.28.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.28.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.29.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.29.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.29.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.29.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.29.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.29.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.29.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.29.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.29.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.30.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.30.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.30.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.30.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.30.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.30.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.30.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.30.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.30.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.layers.31.self_attn.q_proj.weight torch.float32 cpu\n",
      "model.layers.31.self_attn.k_proj.weight torch.float32 cpu\n",
      "model.layers.31.self_attn.v_proj.weight torch.float32 cpu\n",
      "model.layers.31.self_attn.o_proj.weight torch.float32 cpu\n",
      "model.layers.31.mlp.gate_proj.weight torch.float32 cpu\n",
      "model.layers.31.mlp.up_proj.weight torch.float32 cpu\n",
      "model.layers.31.mlp.down_proj.weight torch.float32 cpu\n",
      "model.layers.31.input_layernorm.weight torch.float32 cpu\n",
      "model.layers.31.post_attention_layernorm.weight torch.float32 cpu\n",
      "model.norm.weight torch.float32 cpu\n",
      "lm_head.weight torch.float32 cpu\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype, param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d735a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response(\n",
    "        prompt:str,\n",
    "        tokenizer:AutoTokenizer,\n",
    "        model:AutoModelForCausalLM,\n",
    "        max_length:int=3500,\n",
    "        temperature:float=0.1,\n",
    "        top_k:int=50)->str:\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\",padding=True)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    inputs = {k: v.to(device) for k, v in input_ids.items()}\n",
    "    print(inputs)\n",
    "    attention_mask = input_ids[\"attention_mask\"]\n",
    "    input_ids = input_ids[\"input_ids\"]\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    print(attention_mask[0])\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        output = model.generate(\n",
    "                                    **inputs, \n",
    "                                    max_length=max_length, \n",
    "                                    do_sample=True,\n",
    "                                    temperature=temperature, \n",
    "                                    top_k=top_k,\n",
    "                                    # attention_mask=attention_mask,\n",
    "                                    pad_token_id=pad_token_id,\n",
    "                                    eos_token_id=eos_token_id\n",
    "                                    )\n",
    "        final_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        print(final_output)\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken: {end_time-start_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a11be2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in FP16\n"
     ]
    }
   ],
   "source": [
    "model_fp16 = model.half()\n",
    "tokenizer_fp16 = tokenizer\n",
    "print(\"Model loaded in FP16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7cf963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 1777088000\n",
      "Approximate model size: 3.310084342956543 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of model parameters: {model_fp16.num_parameters()}\")\n",
    "print(f\"Approximate model size: {model_fp16.num_parameters()*2/1024/1024/1024} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d75e312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,   3923,    374,    279,   7438,    315,   2324,     30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "What is the meaning of life? This is a question that has been asked by people throughout history. There are many theories and beliefs about what life's purpose is. Some people believe it's about happiness, others about success, while some look to religion or spirituality for answers. But what does the Bible say about the meaning of life?\n",
      "\n",
      "First, I need to recall what the Bible teaches about life. The Bible is filled with stories and teachings that provide different perspectives. One common theme is that life is a gift from God. In Genesis, it says that God created man in His own image, implying that life is something of great value and purpose.\n",
      "\n",
      "Another perspective is that life is about serving God. In the New Testament, Jesus says, \"I have come to serve, not to be served.\" This suggests that the purpose of life is to live in a way that honors and glorifies God. There's also the idea of being a steward of God's creation, managing the resources and time He has given wisely.\n",
      "\n",
      "Additionally, the Bible talks about life in terms of relationships. It emphasizes the importance of love, both for God and for others. In the book of Proverbs, it's said that love is the foundation of a meaningful life. There's also the idea of community and working together for the common good, as seen in the story of the Good Samaritan and the teachings about neighborly love.\n",
      "\n",
      "The Bible also addresses the concept of purpose through the call to follow Christ. Jesus says, \"Follow Me, and I will make you fishers of men.\" This call suggests that the purpose of life is to share the love and teachings of Christ with others. There's also the idea of being part of God's plan, which is much larger than individual lives, and each person plays a role in this plan.\n",
      "\n",
      "Furthermore, the Bible discusses the afterlife and the eternal aspect of life. It speaks about the importance of eternal life through Jesus Christ. This ties into the idea that life's meaning is not just in the here and now, but also in the future, where we will be with God forever.\n",
      "\n",
      "In summary, the Bible presents several themes about the meaning of life: it's a gift from God, it's about serving Him, it involves relationships and community, it's a call to follow Christ, and it has an eternal perspective. These themes together suggest that life is meant to be lived in a way that is pleasing to God, in love, and in service to others, with the ultimate purpose of glorifying God and enjoying Him forever.\n",
      "</think>\n",
      "\n",
      "The Bible offers a rich and multifaceted perspective on the meaning of life, which can be summarized as follows:\n",
      "\n",
      "1. **Gift from God**: Life is viewed as a precious gift entrusted to humanity by God, as stated in Genesis, where He created man in His own image.\n",
      "\n",
      "2. **Service to God**: The primary purpose of life is to serve God. Jesus' statement, \"I have come to serve, not to be served,\" emphasizes this, suggesting that life is about living in a way that honors and glorifies God.\n",
      "\n",
      "3. **Stewardship**: Life is seen as a stewardship role, where individuals are responsible for managing the resources and time God has entrusted to them wisely.\n",
      "\n",
      "4. **Relationships and Community**: The Bible places a strong emphasis on love and relationships. Love for God and others is central, as seen in the teachings of Proverbs and the parable of the Good Samaritan, which highlights the importance of neighborly love and community.\n",
      "\n",
      "5. **Call to Follow Christ**: The purpose of life is to follow Christ, as He calls individuals to share His love and teachings, becoming fishers of men and spreading His message.\n",
      "\n",
      "6. **God's Plan**: Life is part of a larger plan, with each person playing a role in God's grand narrative, contributing to His purposes on earth.\n",
      "\n",
      "7. **Eternal Perspective**: The Bible introduces an eternal dimension to life, emphasizing the importance of eternal life through Jesus Christ. This perspective extends beyond the present to a future existence with God.\n",
      "\n",
      "In conclusion, the Bible suggests that life is meant to be lived in a manner pleasing to God, characterized by love, service, and community, with the ultimate goal of glorifying God and enjoying Him forever.\n",
      "Time taken: 399.08193278312683\n"
     ]
    }
   ],
   "source": [
    "generate_model_response(\"What is the meaning of life?\", tokenizer=tokenizer, model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
